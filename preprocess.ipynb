{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocess.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "kYsXcnW4Vn52",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rl8q4cGTZLAG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/drive/My Drive/preprocess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OXs5Kz7RWDK9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!touch sentence-split/counter_char"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l7M-ecgQXlxG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w_Ijw92AWLor",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install underthesea"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ntRF2sd2Rj6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cbhT7MW8NBs2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import os\n",
        "from underthesea import sent_tokenize\n",
        "from nltk.tokenize import MWETokenizer, word_tokenize\n",
        "import re\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mEgkR1BVN_Ur",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#text normalize\n",
        "unicode_tohop = ['ẻ', 'é', 'è', 'ẹ', 'ẽ', 'ể', 'ế', 'ề', 'ệ', 'ễ', 'ỷ', 'ý', 'ỳ', 'ỵ', 'ỹ', 'ủ', 'ú', 'ù', 'ụ', 'ũ', 'ử', 'ứ', 'ừ', 'ự', 'ữ', 'ỉ', 'í', 'ì', 'ị', 'ĩ', 'ỏ', 'ó', 'ò', 'ọ', 'õ', 'ở', 'ớ', 'ờ', 'ợ', 'ỡ', 'ổ', 'ố', 'ồ', 'ộ', 'ỗ', 'ả', 'á', 'à', 'ạ', 'ã', 'ẳ', 'ắ', 'ằ', 'ặ', 'ẵ', 'ẩ', 'ấ', 'ầ', 'ậ', 'ẫ', 'Ẻ', 'É', 'È', 'Ẹ', 'Ẽ', 'Ể', 'Ế', 'Ề', 'Ệ', 'Ễ', 'Ỷ', 'Ý', 'Ỳ', 'Ỵ', 'Ỹ', 'Ủ', 'Ú', 'Ù', 'Ụ', 'Ũ', 'Ử', 'Ứ', 'Ừ', 'Ự', 'Ữ', 'Ỉ', 'Í', 'Ì', 'Ị', 'Ĩ', 'Ỏ', 'Ó', 'Ò', 'Ọ', 'Õ', 'Ở', 'Ớ', 'Ờ', 'Ợ', 'Ỡ', 'Ổ', 'Ố', 'Ồ', 'Ộ', 'Ỗ', 'Ả', 'Á', 'À', 'Ạ', 'Ã', 'Ẳ', 'Ắ', 'Ằ', 'Ặ', 'Ẵ', 'Ẩ', 'Ấ', 'Ầ', 'Ậ', 'Ẫ']\n",
        "unicode_dungsan = ['ẻ', 'é', 'è', 'ẹ', 'ẽ', 'ể', 'ế', 'ề', 'ệ', 'ễ', 'ỷ', 'ý', 'ỳ', 'ỵ', 'ỹ', 'ủ', 'ú', 'ù', 'ụ', 'ũ', 'ử', 'ứ', 'ừ', 'ự', 'ữ', 'ỉ', 'í', 'ì', 'ị', 'ĩ', 'ỏ', 'ó', 'ò', 'ọ', 'õ', 'ở', 'ớ', 'ờ', 'ợ', 'ỡ', 'ổ', 'ố', 'ồ', 'ộ', 'ỗ', 'ả', 'á', 'à', 'ạ', 'ã', 'ẳ', 'ắ', 'ằ', 'ặ', 'ẵ', 'ẩ', 'ấ', 'ầ', 'ậ', 'ẫ', 'Ẻ', 'É', 'È', 'Ẹ', 'Ẽ', 'Ể', 'Ế', 'Ề', 'Ệ', 'Ễ', 'Ỷ', 'Ý', 'Ỳ', 'Ỵ', 'Ỹ', 'Ủ', 'Ú', 'Ù', 'Ụ', 'Ũ', 'Ử', 'Ứ', 'Ừ', 'Ự', 'Ữ', 'Ỉ', 'Í', 'Ì', 'Ị', 'Ĩ', 'Ỏ', 'Ó', 'Ò', 'Ọ', 'Õ', 'Ở', 'Ớ', 'Ờ', 'Ợ', 'Ỡ', 'Ổ', 'Ố', 'Ồ', 'Ộ', 'Ỗ', 'Ả', 'Á', 'À', 'Ạ', 'Ã', 'Ẳ', 'Ắ', 'Ằ', 'Ặ', 'Ẵ', 'Ẩ', 'Ấ', 'Ầ', 'Ậ', 'Ẫ']\n",
        "\n",
        "normalizer = dict([('oà','òa'),('oá','óa'),('oả','ỏa'),('oã','õa'),('oạ','ọa'),('oè','òe'),('oé','óe'),('oẻ','ỏe'),('oẽ','õe'),('oẹ','ọe'),('uỳ','ùy'),('uý','úy'),('uỷ','ủy'),('uỹ','ũy'),('uỵ','ụy'),('Uỷ','Ủy')])\n",
        "dic = dict(zip(unicode_tohop, unicode_dungsan)) # Make a pair of 2 lists and convert them to dict()\n",
        "\n",
        "def normalize_unicode(text):\n",
        "  for tohop, dungsan in dic.items():\n",
        "    text = text.replace(tohop, dungsan)\n",
        "  return text\n",
        "\n",
        "def normalize_text(text):\n",
        "  for absurd, normal in normalizer.items():\n",
        "    text = text.replace(absurd, normal)\n",
        "  return text\n",
        "\n",
        "train = os.path.join('sentence-split','splitfile')\n",
        "with open(train) as fopen:\n",
        "  lines = fopen.readlines()\n",
        "  lines = [normalize_text(line) for line in lines]\n",
        "  lines = [normalize_unicode(line) for line in lines]\n",
        "with open(train, 'w') as fwrite:\n",
        "  for line in lines:\n",
        "        fwrite.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lPM-AviM-_ri",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!less sentence-split/splitfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "829evPoHej2a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#append file to file\n",
        "dantri_path = os.path.join('sentence-split','splitfile_dantri')\n",
        "path = os.path.join('sentence-split','splitfile')\n",
        "\n",
        "\n",
        "with open(dantri_path) as fopen, open(path, 'a') as f2:\n",
        "  lines = fopen.readlines()\n",
        "  print(len(lines),lines[-5:-1])\n",
        "  for line in lines:\n",
        "    f2.write(line)\n",
        "\n",
        "with open(path) as f:\n",
        "  line = f.readlines()\n",
        "  print(line[-5:-1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Oa-gfUqyJOg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#buil train/dev/test\n",
        "\n",
        "\n",
        "contain_number_dev = os.path.join('sentence-split','data','contain-number','dev')\n",
        "contain_number_test = os.path.join('sentence-split','data','contain-number','test')\n",
        "contain_number_train = os.path.join('sentence-split','data','contain-number','train')\n",
        "\n",
        "not_contain_number_dev = os.path.join('sentence-split','data','not-contain-number','dev')\n",
        "not_contain_number_test = os.path.join('sentence-split','data','not-contain-number','test')\n",
        "not_contain_number_train = os.path.join('sentence-split','data','not-contain-number','train')\n",
        "\n",
        "def replace_number_tag(string):\n",
        "        return re.sub('[\\d]+', '<number>', string)\n",
        "\n",
        "def read_words():\n",
        "    path = os.path.join('sentence-split','splitfilelower')\n",
        "    with open(path) as f:\n",
        "        lines = f.readlines()\n",
        "        random.shuffle(lines)\n",
        "        print('len lines: ', len(lines))\n",
        "    return lines\n",
        "lines = read_words()\n",
        "\n",
        "with open(not_contain_number_dev, 'w') as fnc, open(contain_number_dev, 'w') as fc:\n",
        "  for i in range(80000):\n",
        "    fc.write(lines[i])\n",
        "    fnc.write(replace_number_tag(lines[i]))\n",
        "    \n",
        "with open(not_contain_number_test, 'w') as fnc, open(contain_number_test, 'w') as fc:\n",
        "  for i in range(80000, 160000):\n",
        "    fc.write(lines[i])\n",
        "    fnc.write(replace_number_tag(lines[i]))\n",
        "    \n",
        "with open(not_contain_number_train, 'w') as fnc, open(contain_number_train, 'w') as fc:\n",
        "  for i in range(160000, 1500191):\n",
        "    fc.write(lines[i])\n",
        "    fnc.write(replace_number_tag(lines[i]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ShDaCYZOknOP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#statistic token\n",
        "train = os.path.join('sentence-split','data','not-contain-number','train')\n",
        "\n",
        "tokenizer = MWETokenizer(separator='')\n",
        "tokenizer.add_mwe(('<','number','>'))\n",
        "\n",
        "def read_words(filename):\n",
        "    with open(train) as f:\n",
        "        lines = f.readlines()\n",
        "        print('len lines: ', len(lines))\n",
        "    i = 0\n",
        "    result = []\n",
        "    for line in lines:\n",
        "      i+=1\n",
        "      if(i%5000==0): print(i)\n",
        "      line = tokenizer.tokenize(word_tokenize(line))\n",
        "      result.extend(line)\n",
        "    return result\n",
        "\n",
        "counter_write = os.path.join('sentence-split','counter')\n",
        "def build_vocab(filename):\n",
        "    data = read_words(filename)\n",
        "    counter = Counter(data)\n",
        "    with open(counter_write, 'w') as f:\n",
        "      f.write(str(counter))\n",
        "    print('vocab length: ', len(counter))\n",
        "\n",
        "build_vocab(train)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DzhUWQ5lSmJB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#statistic character\n",
        "train = os.path.join('sentence-split','data','contain-number','train')\n",
        "raw_text = open(train).read()\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "counter = Counter(raw_text)\n",
        "\n",
        "counter_write = os.path.join('sentence-split','counter_char')\n",
        "with open(counter_write, 'w') as f:\n",
        "    f.write(str(counter))\n",
        "print (\"Total Characters: \", len(raw_text))\n",
        "print (\"Total Vocab: \", len(chars))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ltbMZ7OkWbqY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# sentence tokenize\n",
        "filepath = os.path.join('dantri.txt')\n",
        "savepath = os.path.join('sentence-split','splitfile_dantri')\n",
        "savepahtlower = os.path.join('sentence-split','splitfilelower_dantri')\n",
        "\n",
        "para_number = 0\n",
        "line_number = 0\n",
        "i = 0\n",
        "with open(filepath) as fopen, open(savepath, 'w') as fwrite, open(savepahtlower, 'w') as fwritelower:\n",
        "    paragraphlist = fopen.readlines()\n",
        "    para_number+= len(paragraphlist)\n",
        "    for paragraph in paragraphlist:\n",
        "        i+=1\n",
        "        if(i%1000==0):\n",
        "            print(i)\n",
        "        lines = sent_tokenize(paragraph)\n",
        "        line_number += len(lines)\n",
        "        for line in lines:\n",
        "            fwrite.write(line+'\\n')\n",
        "            fwritelower.write(line.lower()+'\\n')\n",
        "            \n",
        "            \n",
        "print(para_number, line_number)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KxzOtci4sJRZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "plt.rcdefaults()\n",
        "fig, ax = plt.subplots()\n",
        "dic = Counter({',': 0.042353325415595056, '.': 0.0303401183964387, '<number>': 0.02997357244516741, 'của': 0.010395362344106597, 'và': 0.010241053339635526, 'có': 0.009166683102570938, 'là': 0.008068700332823614, 'các': 0.006863054344547401, 'trong': 0.006862392310920782, \"''\": 0.006355246968196318, 'cho': 0.006272989290088929, 'đã': 0.005729707945244853, '``': 0.004349809189495678, 'khi': 0.004276130363799891, 'này': 0.0040176889868085664, '/': 0.0038708830301058413, ')': 0.0036598322268866475, '(': 0.0036113382637368184, 'được': 0.003596635600278992, '-': 0.003583257004074403})\n",
        "people, performance = zip(*dic.items())\n",
        "\n",
        "y_pos = np.arange(len(people))\n",
        "\n",
        "ax.barh(y_pos, performance, align='center', ecolor='black')\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(people)\n",
        "ax.figure.set_size_inches(23, 15)\n",
        "ax.invert_yaxis()  # labels read top-to-bottom\n",
        "ax.set_xlabel('Ratio', fontsize=21)\n",
        "ax.set_title('20 most frequency token',  fontsize=21)\n",
        "\n",
        "plt.tick_params(labelsize=19)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7hsEk82KWKUw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "plt.rcdefaults()\n",
        "fig, ax = plt.subplots()\n",
        "dic = Counter({'space': 0.1926877425460119, 'n': 0.08289039552246273, 'h': 0.06174272107890621, 't': 0.04584348290991594, 'i': 0.040581303056178075, 'c': 0.04057667152034002, 'g': 0.03773725058062532, 'a': 0.03196254346076539, 'o': 0.03063353180345398, '\\\\u0302':0.030117014871949677, 'u': 0.030094373206263122, '\\\\u031b': 0.018797176298593603, 'đ': 0.01791743620326829, 'm': 0.017517626423465726, 'e': 0.015376164090280465, 'r': 0.01336315765527743, 'à': 0.01332224785299052, 'v': 0.01240286911059472, 'l': 0.011627225300368629})\n",
        "people, performance = zip(*dic.items())\n",
        "\n",
        "y_pos = np.arange(len(people))\n",
        "\n",
        "ax.barh(y_pos, performance, align='center', ecolor='black')\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(people)\n",
        "ax.figure.set_size_inches(23, 15)\n",
        "ax.invert_yaxis()  # labels read top-to-bottom\n",
        "ax.set_xlabel('Ratio', fontsize=21)\n",
        "ax.set_title('20 most frequency character',  fontsize=21)\n",
        "\n",
        "plt.tick_params(labelsize=19)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}