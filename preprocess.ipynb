{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocess.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nvlong198/Vietnamese-Diacritics-Restoration/blob/master/preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "kYsXcnW4Vn52",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rl8q4cGTZLAG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/drive/My Drive/preprocess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OXs5Kz7RWDK9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!touch sentence-split/counter_3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l7M-ecgQXlxG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w_Ijw92AWLor",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install underthesea"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ntRF2sd2Rj6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cbhT7MW8NBs2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import os\n",
        "from underthesea import sent_tokenize\n",
        "from nltk.tokenize import MWETokenizer, word_tokenize\n",
        "import re\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mEgkR1BVN_Ur",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#text normalize\n",
        "unicode_tohop = ['ẻ', 'é', 'è', 'ẹ', 'ẽ', 'ể', 'ế', 'ề', 'ệ', 'ễ', 'ỷ', 'ý', 'ỳ', 'ỵ', 'ỹ', 'ủ', 'ú', 'ù', 'ụ', 'ũ', 'ử', 'ứ', 'ừ', 'ự', 'ữ', 'ỉ', 'í', 'ì', 'ị', 'ĩ', 'ỏ', 'ó', 'ò', 'ọ', 'õ', 'ở', 'ớ', 'ờ', 'ợ', 'ỡ', 'ổ', 'ố', 'ồ', 'ộ', 'ỗ', 'ả', 'á', 'à', 'ạ', 'ã', 'ẳ', 'ắ', 'ằ', 'ặ', 'ẵ', 'ẩ', 'ấ', 'ầ', 'ậ', 'ẫ', 'Ẻ', 'É', 'È', 'Ẹ', 'Ẽ', 'Ể', 'Ế', 'Ề', 'Ệ', 'Ễ', 'Ỷ', 'Ý', 'Ỳ', 'Ỵ', 'Ỹ', 'Ủ', 'Ú', 'Ù', 'Ụ', 'Ũ', 'Ử', 'Ứ', 'Ừ', 'Ự', 'Ữ', 'Ỉ', 'Í', 'Ì', 'Ị', 'Ĩ', 'Ỏ', 'Ó', 'Ò', 'Ọ', 'Õ', 'Ở', 'Ớ', 'Ờ', 'Ợ', 'Ỡ', 'Ổ', 'Ố', 'Ồ', 'Ộ', 'Ỗ', 'Ả', 'Á', 'À', 'Ạ', 'Ã', 'Ẳ', 'Ắ', 'Ằ', 'Ặ', 'Ẵ', 'Ẩ', 'Ấ', 'Ầ', 'Ậ', 'Ẫ']\n",
        "unicode_dungsan = ['ẻ', 'é', 'è', 'ẹ', 'ẽ', 'ể', 'ế', 'ề', 'ệ', 'ễ', 'ỷ', 'ý', 'ỳ', 'ỵ', 'ỹ', 'ủ', 'ú', 'ù', 'ụ', 'ũ', 'ử', 'ứ', 'ừ', 'ự', 'ữ', 'ỉ', 'í', 'ì', 'ị', 'ĩ', 'ỏ', 'ó', 'ò', 'ọ', 'õ', 'ở', 'ớ', 'ờ', 'ợ', 'ỡ', 'ổ', 'ố', 'ồ', 'ộ', 'ỗ', 'ả', 'á', 'à', 'ạ', 'ã', 'ẳ', 'ắ', 'ằ', 'ặ', 'ẵ', 'ẩ', 'ấ', 'ầ', 'ậ', 'ẫ', 'Ẻ', 'É', 'È', 'Ẹ', 'Ẽ', 'Ể', 'Ế', 'Ề', 'Ệ', 'Ễ', 'Ỷ', 'Ý', 'Ỳ', 'Ỵ', 'Ỹ', 'Ủ', 'Ú', 'Ù', 'Ụ', 'Ũ', 'Ử', 'Ứ', 'Ừ', 'Ự', 'Ữ', 'Ỉ', 'Í', 'Ì', 'Ị', 'Ĩ', 'Ỏ', 'Ó', 'Ò', 'Ọ', 'Õ', 'Ở', 'Ớ', 'Ờ', 'Ợ', 'Ỡ', 'Ổ', 'Ố', 'Ồ', 'Ộ', 'Ỗ', 'Ả', 'Á', 'À', 'Ạ', 'Ã', 'Ẳ', 'Ắ', 'Ằ', 'Ặ', 'Ẵ', 'Ẩ', 'Ấ', 'Ầ', 'Ậ', 'Ẫ']\n",
        "\n",
        "normalizer = dict([('oà','òa'),('oá','óa'),('oả','ỏa'),('oã','õa'),('oạ','ọa'),('oè','òe'),('oé','óe'),('oẻ','ỏe'),('oẽ','õe'),('oẹ','ọe'),('uỳ','ùy'),('uý','úy'),('uỷ','ủy'),('uỹ','ũy'),('uỵ','ụy'),('Uỷ','Ủy')])\n",
        "dic = dict(zip(unicode_tohop, unicode_dungsan)) # Make a pair of 2 lists and convert them to dict()\n",
        "\n",
        "def compound_unicode(text):\n",
        "  for tohop, dungsan in dic.items():\n",
        "    text = text.replace(tohop, dungsan)\n",
        "  return text\n",
        "\n",
        "def nomalize_text(text):\n",
        "  for absurd, normal in normalizer.items():\n",
        "    text = text.replace(absurd, normal)\n",
        "  return text\n",
        "\n",
        "train = os.path.join('sentence-split','splitfilelower_number')\n",
        "with open(train) as fopen:\n",
        "  lines = fopen.readlines()\n",
        "  lines = [nomalize_text(line) for line in lines]\n",
        "with open(train, 'w') as fwrite:\n",
        "  for line in lines:\n",
        "        fwrite.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "829evPoHej2a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#append file to file\n",
        "dantri_path = os.path.join('sentence-split','splitfile_dantri')\n",
        "path = os.path.join('sentence-split','splitfile')\n",
        "\n",
        "\n",
        "with open(dantri_path) as fopen, open(path, 'a') as f2:\n",
        "  lines = fopen.readlines()\n",
        "  print(len(lines),lines[-5:-1])\n",
        "  for line in lines:\n",
        "    f2.write(line)\n",
        "\n",
        "with open(path) as f:\n",
        "  line = f.readlines()\n",
        "  print(line[-5:-1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Oa-gfUqyJOg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#buil train/dev/test\n",
        "def read_words():\n",
        "    path = os.path.join('sentence-split','splitfilelower_number')\n",
        "    with open(path) as f:\n",
        "        lines = f.readlines()\n",
        "        random.shuffle(lines)\n",
        "        print('len lines: ', len(lines))\n",
        "    return lines\n",
        "lines = read_words()\n",
        "\n",
        "dev = os.path.join('sentence-split','data','dev')\n",
        "test = os.path.join('sentence-split','data','test')\n",
        "train = os.path.join('sentence-split','data','train')\n",
        "\n",
        "with open(dev, 'w') as f:\n",
        "  for i in range(80000):\n",
        "    f.write(lines[i])\n",
        "    \n",
        "with open(test, 'w') as f:\n",
        "  for i in range(80000, 160000):\n",
        "    f.write(lines[i])\n",
        "    \n",
        "with open(train, 'w') as f:\n",
        "  for i in range(160000, 1500191):\n",
        "    f.write(lines[i])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ShDaCYZOknOP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#statistic data\n",
        "train = os.path.join('sentence-split','data','train')\n",
        "\n",
        "tokenizer = MWETokenizer(separator='')\n",
        "tokenizer.add_mwe(('<','number','>'))\n",
        "\n",
        "def read_words(filename):\n",
        "    with open(train) as f:\n",
        "        lines = f.readlines()\n",
        "        print('len lines: ', len(lines))\n",
        "    i = 0\n",
        "    result = []\n",
        "#     with open(save_train_token, 'w') as f:\n",
        "    for line in lines:\n",
        "      i+=1\n",
        "      if(i%2000==0):\n",
        "        print(i)\n",
        "      line = tokenizer.tokenize(word_tokenize(line))\n",
        "#       f.write(' '.join(line) + '\\n')\n",
        "      result.extend(line)\n",
        "    return result\n",
        "\n",
        "counter_write = os.path.join('sentence-split','counter')\n",
        "def build_vocab(filename):\n",
        "    data = read_words(filename)\n",
        "    counter = Counter(data)\n",
        "    with open(counter_write, 'w') as f:\n",
        "      f.write(str(counter))\n",
        "    print('vocab length: ', len(counter))\n",
        "\n",
        "build_vocab(train)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ltbMZ7OkWbqY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# sentence tokenize\n",
        "filepath = os.path.join('dantri.txt')\n",
        "savepath = os.path.join('sentence-split','splitfile_dantri')\n",
        "savepahtlower = os.path.join('sentence-split','splitfilelower_dantri')\n",
        "\n",
        "para_number = 0\n",
        "line_number = 0\n",
        "i = 0\n",
        "with open(filepath) as fopen, open(savepath, 'w') as fwrite, open(savepahtlower, 'w') as fwritelower:\n",
        "    paragraphlist = fopen.readlines()\n",
        "    para_number+= len(paragraphlist)\n",
        "    for paragraph in paragraphlist:\n",
        "        i+=1\n",
        "        if(i%1000==0):\n",
        "            print(i)\n",
        "        lines = sent_tokenize(paragraph)\n",
        "        line_number += len(lines)\n",
        "        for line in lines:\n",
        "            fwrite.write(line+'\\n')\n",
        "            fwritelower.write(line.lower()+'\\n')\n",
        "            \n",
        "            \n",
        "print(para_number, line_number)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KxzOtci4sJRZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.rcdefaults()\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "dic = Counter({',': 0.04234002063070102, '.': 0.030346791393095287, '<number>': 0.029946953645135062, 'của': 0.010395588368583102, 'và': 0.010262971102289452, 'có': 0.009172988466655888, 'là': 0.008069904083624345, 'trong': 0.00687165964054432, 'các': 0.006865674210680401, \"''\": 0.006359836430614001, 'cho': 0.006258801271206006, 'đã': 0.0057268703268480515, '``': 0.004353917530044237, 'khi': 0.004287277905384201, 'này': 0.004014568481123526, '/': 0.003856961355582362, ')': 0.0036570976468544526, '(': 0.0036094348689519095, 'được': 0.0035977122528589806, '-': 0.003573880863907709, 'tại': 0.003523514988647267, 'không': 0.0034031168257163636, 'cũng': 0.0033582950583022244, 'ra': 0.0033246166718328926, 'với': 0.003324202932441377, 'một': 0.0032330423531775434, 'đó': 0.0032320217960118062, 'không': 0.003178870075515162, 'được': 0.003169491982640819, 'người': 0.003157355627156375})\n",
        "people, performance = zip(*dic.items())\n",
        "\n",
        "# Example data\n",
        "# people = ('Tom', 'Dick', 'Harry', 'Slim', 'Jim')\n",
        "y_pos = np.arange(len(people))\n",
        "# performance = 3 + 10 * np.random.rand(len(people))\n",
        "# error = np.random.rand(len(people))\n",
        "\n",
        "ax.barh(y_pos, performance, align='center', ecolor='black')\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(people)\n",
        "ax.figure.set_size_inches(17, 15)\n",
        "ax.invert_yaxis()  # labels read top-to-bottom\n",
        "ax.set_xlabel('Ratio')\n",
        "ax.set_title('30 most frequency token')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9IjyBswlwEV5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def set_size(w,h, ax=None):\n",
        "    \"\"\" w, h: width, height in inches \"\"\"\n",
        "    if not ax: ax=plt.gca()\n",
        "    l = ax.figure.subplotpars.left\n",
        "    r = ax.figure.subplotpars.right\n",
        "    t = ax.figure.subplotpars.top\n",
        "    b = ax.figure.subplotpars.bottom\n",
        "    figw = float(w)/(r-l)\n",
        "    figh = float(h)/(t-b)\n",
        "    ax.figure.set_size_inches(figw, figh)\n",
        "\n",
        "fig, ax=plt.subplots()\n",
        "\n",
        "ax.plot([1,3,2])\n",
        "\n",
        "set_size(5,5)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}